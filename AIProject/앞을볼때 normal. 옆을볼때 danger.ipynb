{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fa3cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\project\\deep\\lib\\site-packages (4.7.0.68)\n",
      "Requirement already satisfied: dlib in c:\\project\\deep\\lib\\site-packages (19.24.6)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\project\\deep\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\project\\deep\\lib\\site-packages\\radam-0.0.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python dlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b0404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\project\\deep\\lib\\site-packages (4.7.0.68)\n",
      "Requirement already satisfied: dlib in c:\\project\\deep\\lib\\site-packages (19.24.6)\n",
      "Requirement already satisfied: imutils in c:\\project\\deep\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\project\\deep\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\project\\deep\\lib\\site-packages\\radam-0.0.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python dlib imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ee5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 카메라 행렬 설정\n",
    "카메라 행렬(camera_matrix)의 값이 실제 카메라의 초점 거리와 센서 크기에 맞지 않으면 정확한 포즈 추정이 어렵습니다. 현재는 화면의 가로 크기를 초점 거리로 사용하고 있지만, 실제 카메라의 매개변수를 사용해야 합니다.\n",
    "\n",
    "2. 왜곡 계수\n",
    "왜곡 계수(dist_coeffs)를 0으로 설정했는데, 실제 카메라의 왜곡 계수를 고려해야 할 수 있습니다. 실제 왜곡 계수를 모르더라도, 0으로 설정하는 것은 기본값입니다.\n",
    "\n",
    "3. 오일러 각도 추출\n",
    "cv2.decomposeProjectionMatrix를 사용하여 오일러 각도를 추출하는 방법은 좀 복잡할 수 있습니다. 오일러 각도를 직접 계산하거나 다른 방법으로 포즈를 추정할 수 있습니다.\n",
    "\n",
    "4. PnP 알고리즘의 정확성\n",
    "cv2.solvePnP의 결과가 정확하지 않을 수 있습니다. 모델 포인트와 이미지 포인트의 매칭이 잘못되었을 수 있습니다.\n",
    "\n",
    "아래는 개선된 코드로, 카메라 매트릭스와 왜곡 계수를 좀 더 정확하게 설정하고, 회전 벡터에서 직접 피치와 요 각도를 추정하는 방법을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b62c9f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\project\\deep\\lib\\site-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\project\\deep\\lib\\site-packages (0.18.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\project\\deep\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\project\\deep\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\project\\deep\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\project\\deep\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\project\\deep\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\project\\deep\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\project\\deep\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\project\\deep\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\project\\deep\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\project\\deep\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\project\\deep\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\project\\deep\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\project\\deep\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\project\\deep\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\project\\deep\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\project\\deep\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\project\\deep\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\project\\deep\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\project\\deep\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\project\\deep\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: colorama in c:\\project\\deep\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\project\\deep\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\project\\deep\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\project\\deep\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\project\\deep\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\project\\deep\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\project\\deep\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\project\\deep\\lib\\site-packages\\radam-0.0.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d916e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\project\\deep\\lib\\site-packages (4.7.0.68)\n",
      "Requirement already satisfied: dlib in c:\\project\\deep\\lib\\site-packages (19.24.6)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\project\\deep\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\project\\deep\\lib\\site-packages\\radam-0.0.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python dlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab1ae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\project\\deep\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe Hands 모듈 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def is_palm_facing_camera(hand_landmarks):\n",
    "    \"\"\"손바닥이 카메라를 향하고 있는지 확인하는 함수\"\"\"\n",
    "    wrist = np.array([hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].x,\n",
    "                      hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].y,\n",
    "                      hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].z])\n",
    "    \n",
    "    index_finger_tip = np.array([hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x,\n",
    "                                 hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y,\n",
    "                                 hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].z])\n",
    "    \n",
    "    thumb_tip = np.array([hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].x,\n",
    "                          hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y,\n",
    "                          hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].z])\n",
    "    \n",
    "    # 손목과 손끝 사이의 벡터를 계산\n",
    "    wrist_to_index = index_finger_tip - wrist\n",
    "    wrist_to_thumb = thumb_tip - wrist\n",
    "    \n",
    "    # 벡터의 내적을 사용하여 손바닥의 방향 판단\n",
    "    palm_normal_vector = np.cross(wrist_to_index, wrist_to_thumb)\n",
    "    \n",
    "    # 벡터의 z값을 확인 (손바닥이 카메라를 향할 때 z값이 양수임)\n",
    "    return palm_normal_vector[2] > 0\n",
    "\n",
    "# 카메라 캡처\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # BGR 이미지를 RGB로 변환\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # MediaPipe로 손 랜드마크 검출\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    hand_detected = False\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            hand_detected = True\n",
    "\n",
    "            # 손바닥이 카메라를 향하고 있는지 판단\n",
    "            if is_palm_facing_camera(hand_landmarks):\n",
    "                color = (0, 255, 0)  # 초록색 (정상)\n",
    "                status = \"NORMAL\"\n",
    "            else:\n",
    "                color = (0, 0, 255)  # 빨간색 (비정상)\n",
    "                status = \"DANGER\"\n",
    "\n",
    "            # 손 랜드마크를 이미지에 그림\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                      mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=2),\n",
    "                                      mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=2))\n",
    "\n",
    "            # 바운딩 박스 그리기\n",
    "            h, w, c = frame.shape\n",
    "            x_min = min([landmark.x for landmark in hand_landmarks.landmark]) * w\n",
    "            y_min = min([landmark.y for landmark in hand_landmarks.landmark]) * h\n",
    "            x_max = max([landmark.x for landmark in hand_landmarks.landmark]) * w\n",
    "            y_max = max([landmark.y for landmark in hand_landmarks.landmark]) * h\n",
    "            cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), color, 2)\n",
    "\n",
    "            # 상태 텍스트 표시\n",
    "            cv2.putText(frame, status, (int(x_min), int(y_min) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    if not hand_detected:\n",
    "        # 손이 검출되지 않은 경우 메시지 표시하지 않음\n",
    "        pass\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662927aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1390c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in c:\\project\\deep\\lib\\site-packages (0.5.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\project\\deep\\lib\\site-packages\\radam-0.0.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909fe117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab58125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading facial landmark predictor...\n",
      "[INFO] starting video stream thread...\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n",
      "Alert, concentrate on driving !!!\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python detect_drowsiness.py --shape-predictor shape_predictor_68_face_landmarks.dat\n",
    "# python detect_drowsiness.py --shape-predictor shape_predictor_68_face_landmarks.dat --alarm alarm.wav\n",
    "\n",
    "# import the necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "# import playsound\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# def sound_alarm(path):\n",
    "# \t# play an alarm sound\n",
    "# \tplaysound.playsound(path)\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "\t# compute the euclidean distances between the two sets of\n",
    "\t# vertical eye landmarks (x, y)-coordinates\n",
    "\tA = dist.euclidean(eye[1], eye[5])\n",
    "\tB = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "\t# compute the euclidean distance between the horizontal\n",
    "\t# eye landmark (x, y)-coordinates\n",
    "\tC = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "\t# compute the eye aspect ratio\n",
    "\tear = (A + B) / (2.0 * C)\n",
    "\n",
    "\t# return the eye aspect ratio\n",
    "\treturn ear\n",
    " \n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-p\", \"--shape-predictor\", required=True,\n",
    "# \thelp=\"path to facial landmark predictor\")\n",
    "# ap.add_argument(\"-a\", \"--alarm\", type=str, default=\"\",\n",
    "# \thelp=\"path alarm .WAV file\")\n",
    "# ap.add_argument(\"-w\", \"--webcam\", type=int, default=0,\n",
    "# \thelp=\"index of webcam on system\")\n",
    "# args = vars(ap.parse_args())\n",
    " \n",
    "# define two constants, one for the eye aspect ratio to indicate\n",
    "# blink and then a second constant for the number of consecutive\n",
    "# frames the eye must be below the threshold for to set off the\n",
    "# alarm\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 48\n",
    "\n",
    "# initialize the frame counter as well as a boolean used to\n",
    "# indicate if the alarm is going off\n",
    "COUNTER = 0\n",
    "ALARM_ON = False\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# grab the indexes of the facial landmarks for the left and\n",
    "# right eye, respectively\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "# start the video stream thread\n",
    "print(\"[INFO] starting video stream thread...\")\n",
    "vs = VideoStream(0).start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "# loop over frames from the video stream\n",
    "count = 0\n",
    "while True:\n",
    "\t# grab the frame from the threaded video file stream, resize\n",
    "\t# it, and convert it to grayscale\n",
    "\t# channels)\n",
    "\tframe = vs.read()\n",
    "\tframe = imutils.resize(frame, width=450)\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\t# detect faces in the grayscale frame\n",
    "\trects = detector(gray, 0)\n",
    "\tif len(rects) == 0:\n",
    "\t\tcount += 1\n",
    "\t\tif count >= 30:\n",
    "\t\t\tprint('Alert, concentrate on driving !!!')\n",
    "\telse:\n",
    "\t\tcount = 0\n",
    "\n",
    "\t# loop over the face detections\n",
    "\tfor rect in rects:\n",
    "\t\t# determine the facial landmarks for the face region, then\n",
    "\t\t# convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "\t\t# array\n",
    "\t\tshape = predictor(gray, rect)\n",
    "\t\tshape = face_utils.shape_to_np(shape)\n",
    "\n",
    "\t\t# extract the left and right eye coordinates, then use the\n",
    "\t\t# coordinates to compute the eye aspect ratio for both eyes\n",
    "\t\tleftEye = shape[lStart:lEnd]\n",
    "\t\trightEye = shape[rStart:rEnd]\n",
    "\t\tleftEAR = eye_aspect_ratio(leftEye)\n",
    "\t\trightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "\t\t# average the eye aspect ratio together for both eyes\n",
    "\t\tear = (leftEAR + rightEAR) / 2.0\n",
    "\t\t# print(ear.dtype)\n",
    "\n",
    "\t\t# compute the convex hull for the left and right eye, then\n",
    "\t\t# visualize each of the eyes\n",
    "\t\tleftEyeHull = cv2.convexHull(leftEye)\n",
    "\t\trightEyeHull = cv2.convexHull(rightEye)\n",
    "\t\tcv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "\t\tcv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "\t\t# check to see if the eye aspect ratio is below the blink\n",
    "\t\t# threshold, and if so, increment the blink frame counter\n",
    "\t\tif ear < EYE_AR_THRESH:\n",
    "\t\t\tCOUNTER += 1\n",
    "\n",
    "\t\t\t# if the eyes were closed for a sufficient number of\n",
    "\t\t\t# then sound the alarm\n",
    "\t\t\tif COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "\t\t\t\t# if the alarm is not on, turn it on\n",
    "\t\t\t\tif not ALARM_ON:\n",
    "\t\t\t\t\tALARM_ON = True\n",
    "\n",
    "\t\t\t\t\t# check to see if an alarm file was supplied,\n",
    "\t\t\t\t\t# and if so, start a thread to have the alarm\n",
    "\t\t\t\t\t# sound played in the background\n",
    "\t\t\t\t\t# if args[\"alarm\"] != \"\":\n",
    "\t\t\t\t\t# \tt = Thread(target=sound_alarm,\n",
    "\t\t\t\t\t# \t\targs=(args[\"alarm\"],))\n",
    "\t\t\t\t\t# \tt.deamon = True\n",
    "\t\t\t\t\t# \tt.start()\n",
    "\n",
    "\t\t\t\t# draw an alarm on the frame\n",
    "\t\t\t\tcv2.putText(frame, \"DROWSINESS ALERT!\", (10, 30),\n",
    "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "\t\t# otherwise, the eye aspect ratio is not below the blink\n",
    "\t\t# threshold, so reset the counter and alarm\n",
    "\t\telse:\n",
    "\t\t\tCOUNTER = 0\n",
    "\t\t\tALARM_ON = False\n",
    "\n",
    "\t\t# draw the computed eye aspect ratio on the frame to help\n",
    "\t\t# with debugging and setting the correct eye aspect ratio\n",
    "\t\t# thresholds and frame counters\n",
    "\t\t# cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
    "\t\t# \tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    " \n",
    "\t# show the frame\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
